language: python
python: "3.6"
env:
  global:
    - SPARK_HOME=spark-2.3.1-bin-hadoop2.7
    - PYTHONPATH=anndata:scanpy
git:
  submodules: false
before_install:
  - sed -i 's/git@github.com:/https:\/\/github.com\//' .gitmodules
  - git submodule update --init --recursive
install:
  - pip install -r requirements.txt
  - |
    if [ ! -f "$SPARK_HOME/bin/spark-submit" ]; then
      wget http://www-us.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz
      tar xf spark-2.3.1-bin-hadoop2.7.tgz
    else
      echo "Found existing Spark"
    fi
script: python test_scanpy_spark.py
cache: pip
cache:
  directories:
    - spark-2.3.1-bin-hadoop2.7
    - /home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages
